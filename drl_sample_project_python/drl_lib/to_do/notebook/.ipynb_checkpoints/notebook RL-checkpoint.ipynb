{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toe\n",
    "<br>\n",
    "3^9 = 19683 états possibles mais seulement 5478 états valides\n",
    "<br><br>\n",
    "\n",
    "## Monte Carlo\n",
    "<br>\n",
    "\n",
    "### Monte Carlo Exploring starts\n",
    "\n",
    "Différents tests avec 30 000 itérations  \n",
    "gamma vaut 0.999999 afin que l'agent favorise les victoires directes aux victoires qu'il pourrait penser obtenir à coup sur quelques coups plus tard\n",
    "\n",
    "reset random:  \n",
    "```python\n",
    "\n",
    "    def reset_random(self, p_random):\n",
    "        self.player_turn = 'x'\n",
    "        self.board = ['*'] * 9\n",
    "        if random.uniform(0, 1) < p_random:\n",
    "            nb_random = random.randint(1, 8)\n",
    "            while nb_random > 0:\n",
    "                nb_random -= 1\n",
    "                action_chosen = random.choice(self.available_action_ids())\n",
    "                self.act_with_action_id(action_chosen)\n",
    "                if self.is_game_over():\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "```\n",
    "p_random étant la probabilité de générer un board randomisé\n",
    "\n",
    "le score est calculé en faisant nb_victoires - nb_défaites\n",
    "\n",
    "<br><br>\n",
    "#### Premier test\n",
    "<br>\n",
    "10 % de chances de faire un reset avec un board randomisé, au bout de 25 000 itérations politique greedy. (test de l'agent)\n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_1.png\">\n",
    "(en ordonnées un % de victoire/défaite/draws et en abscisse les itérations regroupées en 100 par 100)  \n",
    "\n",
    "3151 états visités  \n",
    "Sur les 1000 dernières itérations:  \n",
    "86.5 % de victoires  \n",
    "3.0 % de défaites  \n",
    "10.5 % de nulles\n",
    "<br><br>\n",
    "-> trop peu d'états visités\n",
    "<br><br>\n",
    "#### Second test\n",
    "<br>\n",
    "30 % de chances de faire un reset avec un board randomisé, au bout de 25 000 itérations politique greedy.\n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_2.png\">\n",
    "\n",
    "4135 états visités  \n",
    "Sur les 1000 dernières itérations:  \n",
    "87.3 % de victoires  \n",
    "6.8 % de défaites  \n",
    "5.9 % de nulles  \n",
    "<br>\n",
    "-> plus d'états visités mais des résultats assez identiques  \n",
    "<br><br>\n",
    "#### Trosième test\n",
    "\n",
    "100 % de chances de faire un board randomisé les 10 000 premières itérations. \n",
    "50 % de chance de faire un board randomisé de 10 000 à 20 000 itérations.  \n",
    "30 % de chance de faire un board randomisé de 20 000 à 25 000 itérations.  \n",
    "0 % de chance de faire un board randomisé après 25 000 itérations.  \n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_3.png\">\n",
    "\n",
    "4463 états visités  \n",
    "Sur les 1000 dernières itérations:  \n",
    "80.1 % de victoires  \n",
    "7.9 % de défaites  \n",
    "11.9 % de nulles\n",
    "<br><br>\n",
    "-> nombre d'états visités non coréllés au score de l'agent, possible que le random ne donne pas la véritable valeur de Q. Dans une position ou il y aurait une victoire forcé et 2 possibilités de défaites en cas d'action non optimale, l'agent en jouant du random perdrait 2 fois sur 3 et considérerait la position commé étant mauvaise.\n",
    "<br><br>\n",
    "#### Quatrième test\n",
    "\n",
    "On commence avec 100 % (p) de chance de faire un board randomisé et à chaque itération on multiplie p par 0.999905 jusqu'à la 25000ème itération\n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_4.png\">\n",
    "\n",
    "4241 états visités. \n",
    "Sur les 1000 dernières itérations:  \n",
    "86.8 % de victoires  \n",
    "5.2 % de défaites  \n",
    "8.0 % de nulles\n",
    "<br><br>\n",
    "-> ecnore une fois peu de changement\n",
    "<br><br>\n",
    "#### Cinquième test\n",
    "\n",
    "On garde le learning rate adaptatif de la dernière fois mais on change la fonction score  \n",
    "nouvelle fonction score: nb_victoires - 10 x nb_defaites\n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_5.png\">\n",
    "\n",
    "4301 états visités.  \n",
    "Sur les 1000 dernières itérations:  \n",
    "91.1 % de victoires  \n",
    "0 % de défaites  \n",
    "8.9 % de nulles\n",
    "<br><br>\n",
    "-> en punissant énormément la défaite, l'agent préfère ne prendre aucun risque de perdre là ou avant il aurait pris le risque s'il avait légèrement plus de chances de gagner. Ce run est bénie mais certains runs donnent des taux de défaites aux alentours de 0.5%. Pour régler le soucis il faut entrainer le modèle plus longtemps, 50 000 itérations garantissent de ne perdre aucune partie.\n",
    "<br><br>\n",
    "Il est également possible de placer le joueur random en second, auquel cas les résultats sont nettement moins bons même avec bien plus d'itérations. Test avec 600k itérations: \n",
    "\n",
    "<img src=\"imgs/monte_carlo_es/monte_carlo_es_6.png\">\n",
    "(Les résultats sont regroupés par tranche de 1000 pour une meilleure lisibilité du grahique)  \n",
    "\n",
    "4520 états visités.  \n",
    "Sur les 10000 dernières itérations:  \n",
    "71.61 % de victoires  \n",
    "5.18 % de défaites  \n",
    "23.21 % de nulles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
